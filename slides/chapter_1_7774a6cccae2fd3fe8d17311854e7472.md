---
title: Insert title here
key: 7774a6cccae2fd3fe8d17311854e7472

---
## Screencast

```yaml
type: "TitleSlide"
key: "be5c7a4595"
```

`@lower_third`

name: Christopher Bruffaerts
title: Lecturer in Data science, EPFL


`@script`
In this lesson, we cover the most popular algorithm of the class of association rule mining methods, the so-called apriori algorithm.


---
## Association rule mining

```yaml
type: "FullSlide"
key: "c825ec5187"
```

`@part1`
**Association rule mining** allows to discover interesting relationships between items in a large transactional database.

This mining task can be divided into two subtasks:

- **Frequent itemset generation**: determine all frequent itemsets of a potentially large database of transactions. An itemset is said to be frequent if it satisfies a _minimum support threshold_.

- **Rule generation**: from the above frequent itemsets, generate association rules with confidence above a _minimum confidence threshold_.

&nbsp;

The **Apriori algorithm** is a classic and fast mining algorithm belonging to the class of association rule mining algorithms.


`@script`
To be able to extract induced rules from the transactional dataset, the association rule mining can be decomposed into two subtasks. First, the set of frequent items has to be generated from the database of transactions. An itemset is considered as frequent if its support satisfies a minimum threshold value. 
Second, using the set of frequent itemsets, rules with high confidence are extracted.

Note that the frequent itemset generation is much more computationally intensive than the generation of the associated rules.


---
## Back to the transactional data

```yaml
type: "TwoColumns"
key: "1813478938"
hide_title: false
```

`@part1`
| TID 	 | Transaction    |
|--------|----------------|
| 1      | {A, B, C, D}   |
| 2      | {A, B, D}      |
| 3      | {A, B}         |
| 4      | {B, C, D}      |
| 5      | {B, C}         |
| 6      | {C, D}         |
| 7      | {B, D}         |


`@part2`
![itemset_lattice](https://assets.datacamp.com/production/repositories/4926/datasets/d7997ea66cddc7c86cc7a84319b8cc640a6a4643/itemset_lattice.png =110){{2}}


`@script`
To illustrate how the apriori algorithm works, we use the transactional dataset seen previously and its corresponding itemset lattice.


---
## Idea behind the Apriori algorithm

```yaml
type: "FullSlide"
key: "0a6b566e2c"
```

`@part1`
**The apriori algorithm:**

- Level-wise, breath-first algorithm which counts transactions
- Generates candidate itemsets by exploiting the apriori principle

**Apriori principle**:

- If an itemset is frequent, then all of its subsets must also be frequent.
	- _e.g._ if {A,B} is frequent, then both {A} and {B} are frequent
- For an infrequent itemset, all its super-sets are infrequent.
	- _e.g._ if {A} is infrequent, then {A,B}, {A,C} and {A,B,C} are infrequent.


`@citations`
Agrawal and Srikant (1994)


`@script`
Level-wise approach as it traverses the itemset lattice one at a time, from frequent 1-itemset to the maximum size of frequent itemsets. 

Breadth-first algorithm searches the graph data structure. It starts at the tree root and explores all of the neighbor nodes at the present depth prior to moving on to the nodes at the next depth level.

The apriori algorithm generates candidate itemsets and measures support values in an efficient manner, via in particular the apriori principle.

The apriori principle states that:
- if an itemset is frequent, then all of its subsets must also be frequent. If the itemset {A,B} is frequent, then both {A} and {B} are frequent.
- Likewise, if an itemset is infrequent, then all its super-sets are infrequent. If itemset {A} is infrequent, then itemsets {A,B}, {A,C} and {A,B,C} are infrequent.


---
## Recap of the apriori algorithm

```yaml
type: "FullSlide"
key: "def53eb7c2"
```

`@part1`
Given a set of transactions, find all rules having at the same time

- support >= _minimum support threshold _
- confidence >= _minimum confidence threshold_


candidate generation
candidate pruning

It has got this odd name because it uses â€˜priorâ€™ knowledge of frequent itemset properties.


`@script`
To avoid performing useless computations, it is useful to prune the rules without having to compute support and confidence values.


---
## Example: 1-itemset

```yaml
type: "TwoColumns"
key: "867fc1190a"
```

`@part1`
![item_set_lattice2](https://assets.datacamp.com/production/repositories/4926/datasets/2410e69c0cc858c2d970f83b1d36b98daf3f7b6e/itemset_lattice_1_r.png =80)


`@part2`
| TID 	 | Transaction                            |
|--------|----------------------------------------|
| 1      | {A, B, C, D}     	  |
| 2      | {A, B, D}              |
| 3      | {A, B}                |
| 4      | {B, C, D}         |
| 5      | {B, C}         |
| 6      | {C, D}         |
| 7     | {B, D}         |


`@citations`
Minimum support threshold = 3/7 = 0.42


`@script`
Let us reuse the lattice structure for that example. The search space of itemsets that need to be explored is exponentially large.

The apriori algorithm is a level-wise algorithm, meaning that we first start with the 1-itemset, the itemsets of size 1. 
We start by counting the number of occurences of A within the transactional dataset; in this case it appears 3 times. Similarly, we scan the dataset to obtain the support of items B, C and D. In this case, all support values satisfy the minimum support threshold which is set to 3. Therefore, all 4 itemsets are considered as frequent itemsets. This means that all 4 itemsets will be used to generate further candidate itemsets.


---
## Example: 2-itemsets

```yaml
type: "TwoColumns"
key: "0d9dabb41c"
```

`@part1`
![item_set_lattice3](https://assets.datacamp.com/production/repositories/4926/datasets/c47cab96e3fe4710525fcece8c9299b86feb52b7/itemset_lattice_2_r.png =80)


`@part2`
| TID 	 | Transaction                            |
|--------|----------------------------------------|
| 1      | {A, B, C, D}     	  |
| 2      | {A, B, D}              |
| 3      | {A, B}                |
| 4      | {B, C, D}         |
| 5      | {B, C}         |
| 6      | {C, D}         |
| 7      | {B, D}         |


`@citations`
Minimum support threshold = 3/7 = 0.42


`@script`
Once the first level (itemsets of length 1) has been analyzed, we move on to the second level (itemsets of length 2). All candidates are generated based on the frequent 1-itemsets. The support values of the 6 candidates are shown on the graph. Both itemsets (AC) and (AD) do not satisfy the minimum support threshold. From the apriori principle, this means that all supersets of (AC) and (AD) cannot be considered as frequent itemsets.


---
## Example: 3-itemsets

```yaml
type: "TwoColumns"
key: "5608bd3d6f"
```

`@part1`
![all_itemsets](https://assets.datacamp.com/production/repositories/4926/datasets/a95bdc27e475bdb72214862125361e91c975bb48/itemset_lattice_2b_r.png =75)


`@part2`
| TID 	 | Transaction                            |
|--------|----------------------------------------|
| 1      | {A, B, C, D}     	  |
| 2      | {A, B, D}              |
| 3      | {A, B}                |
| 4      | {B, C, D}         |
| 5      | {B, C}         |
| 6      | {C, D}         |
| 7      | {B, D}         |


`@citations`
Minimum support threshold = 3/7 = 0.42


`@script`
As both itemsets (AC) and (AD) are infrequent, they are colored in orange on the graph. Their respective supersets are as well colored in orange as these super-sets are themselves infrequent itemsets (based on the apriori algorithm). It is therefore not necessary to scan the database for itemsets that are supersets of both infrequent itemsets, which substantially reduces the exploration space. 

The only 3-itemset candidate remaining is the itemset {BCD} which has a support of 2, meaning that it is an infrequent itemset.


---
## Example: frequent itemsets

```yaml
type: "TwoColumns"
key: "bc082b2107"
```

`@part1`
![item_set_lattice3](https://assets.datacamp.com/production/repositories/4926/datasets/dd85eef5519ce0bde80f9cf799c167d939b2a563/itemset_lattice_3_r.png =78)


`@part2`
| Itemset| Count  | Support |
|--------|--------|---------|
| {A}    | 3      | 0.42
| {B}    | 6      | 0.85
| {C}    | 4      | 0.57
| {D}    | 5      | 0.71
| {A,B}  | 3 	  | 0.42
| {B,C}  | 3      | 0.42
| {B,D}  | 4      | 0.57
| {C,D}  | 3      | 0.42


`@citations`
Minimum support threshold = 3/7 = 0.42


`@script`
The itemset {ABCD} at the bottom of the tree has been already discarded based on the apriori principle.
We finally obtain the setof frequent itemsets, shown in blue on the graph. This example has helped to understand how the apriori algorithm works to determine efficiently the frequent itemsets.


---
## A first try with the apriori

```yaml
type: "FullSlide"
key: "a6ae55fdc5"
```

`@part1`
**Transactional data**
```
inspect(head(trans,2))
```

```
    items     transactionID
[1] {A,B,C,D} 1            
[2] {A,B,D}   2            
```

**First call to the apriori function - frequent itemsets**
```r
support.all = apriori(trans, 
                      parameter = list(supp = 3/7, target="frequent itemsets"))
```


`@script`
Let us give a first try with the apriori function of the arules package!

Remember that the data we are working with is a transactional type of dataset, or at least one that can be coerced to such data type structure. The "transactional" object denoted "trans" contains the dataset shown in the previous slides. The first two transactions are displayed on the slide.

Let us start with a standard call of the apriori function to retrieve the list of frequent itemsets. We set as parameters of the function: 1. the minimum support threshold denoted as "supp" and 2. the "target" as frequent itemset. We assign the object to "support.all".


---
## Output of the apriori - frequent itemsets

```yaml
type: "TwoColumns"
key: "a9f5eecee4"
```

`@part1`
**Frequent itemsets**
```
> inspect(support.all)
    items support   count
[1] {A}   0.4285714 3    
[2] {C}   0.5714286 4    
[3] {D}   0.7142857 5    
[4] {B}   0.8571429 6    
[5] {A,B} 0.4285714 3    
[6] {C,D} 0.4285714 3    
[7] {B,C} 0.4285714 3    
[8] {B,D} 0.5714286 4
```


`@part2`
![item_set_lattice3](https://assets.datacamp.com/production/repositories/4926/datasets/dd85eef5519ce0bde80f9cf799c167d939b2a563/itemset_lattice_3_r.png =78)


`@script`
By inspecting the object, we obtain a dataframe containing the set of frequent itemsets which corresponds to the itemset lattice we built earlier.


---
## Extracting rules with the apriori function

```yaml
type: "FullSlide"
key: "300fe878ef"
```

`@part1`
**Parameter**: the mining parameters change the characteristics of the mined itemsets or rules.
- Support = 3/7
- Confidence = 60%
- Minimum length of rule = 2

**Call to the apriori function for rule generation with specific arguments**
```
rules.all = apriori(trans,
				parameter = list(supp=3/7, conf=0.6, minlen=2),
                control = list(verbose=F),
                appearance = list(rhs="A", default = "lhs"))
inspect(rules.all)
```


`@script`
To generate association rules, we call the apriori function with as argument the transactional datasets as well as the following parameters passed in a list:
- minimum support threshold
- confidence threshold
- minimum length of rules 

to steer the output and the set of extracted rules.

Sometimes, you may be interested in a specific rule, such as imposing that the consequent of the rule is A.


---
## Extracting rules: output

```yaml
type: "FullSlide"
key: "7acad20eeb"
```

`@part1`
**Inspecting the rules** 

```
> inspect(rules.all)
    lhs    rhs support   confidence lift      count
[1] {A} => {B} 0.4285714 1.0000000  1.1666667 3    
[2] {C} => {D} 0.4285714 0.7500000  1.0500000 3    
[3] {D} => {C} 0.4285714 0.6000000  1.0500000 3    
[4] {C} => {B} 0.4285714 0.7500000  0.8750000 3    
[5] {D} => {B} 0.5714286 0.8000000  0.9333333 4    
[6] {B} => {D} 0.5714286 0.6666667  0.9333333 4 
```


`@script`
The "inspect" function allows to extract the generate rules with their corresponding measures.
For instance, the first rule states that "A implies B" for a support of 42% and a confidence of 100%.


---
## Rule redundancy

```yaml
type: "FullSlide"
key: "d9c318cd97"
```

`@part1`
**Set of generated rules**

```
rules = apriori(trans,control = list(verbose=F),
                parameter = list(supp=0.05, conf=0.5, minlen=2),
                appearance = list(rhs="A", default = "lhs"))
 
```

**Set of pruned rules (non-redundant)**

```
subset.mat = is.subset(rules, rules)
redundant = colSums(subset.mat)>1
rules.pruned = rules[!redundant]
```


`@script`
There are often too many association rules discovered from a transactional dataset. To illustrate this, let us call again the apriori function with different parameters. 

Given the large number of rules extracted from the apriori function, it is necessary to get an overview of redundant rules and identify the interesting rules. 

We use the "is.subset" function to figure out which items are subsets of which items among all possible itemsets. This allows to obtain the list of redundant rules, and exclude the redundant rules from the set or original extracted rules.


---
## Rule redundancy

```yaml
type: "FullSlide"
key: "3a81639fe1"
```

`@part1`
**Comparing extracted rules and non-redundant rules**

```
> inspect(rules) 
    lhs        rhs support   confidence lift     count
[1] {B}     => {A} 0.4285714 0.5        1.166667 3    
[2] {B,D}   => {A} 0.2857143 0.5        1.166667 2    
[3] {B,C,D} => {A} 0.1428571 0.5        1.166667 1 
```

```
> inspect(rules.pruned)
    lhs    rhs support   confidence lift     count
[1] {B} => {A} 0.4285714 0.5        1.166667 3 
```


`@script`
In this example, rule #2 and rule #3 are considered redundant rules as they provide no extra knowledge in addition to rule #1.
When a rule (such as #2 and #3) is a super rule of another rule (#1) and the former has the same or a lower lift, the former rule is considered to be redundant.
In this case, if "B implies A", it is natural that both "BD" and "BCD" also imply A (given that the lift is the same, 1.16). This means that the rule "B implies A" is a non-redundant rule, the rule considered as the interesting one.


---
## Let's practice!

```yaml
type: "FinalSlide"
key: "cbc4d2b5e4"
```

`@script`
Now it is your turn to practice the skills you have learned on the Online Retail dataset. You will use the transactional dataset to apply the apriori algorithm. Happy shopping!

