---
title: Insert title here
key: 7774a6cccae2fd3fe8d17311854e7472

---
## Screencast

```yaml
type: "TitleSlide"
key: "be5c7a4595"
```

`@lower_third`

name: Christopher Bruffaerts
title: Lecturer in Data science, EPFL


`@script`
In this lesson, we are going to cover one of most popular algorithm of the class of association rule mining methods, the so-called apriori algorithm. We are first going to understand the rationale behind this algorithm and afterwards see how to apply it with the arules package.


---
## Association rule mining

```yaml
type: "FullSlide"
key: "c825ec5187"
```

`@part1`
**Association rule mining** allows to discover interesting relationships between items in a large transactional database.

This mining task can be divided into two subtasks:

- **Frequent itemset generation**: determine all frequent itemsets of a potentially large database of transactions. An itemset is said to be frequent if it satisfies a _minimum support threshold_.

- **Rule generation**: from the above frequent itemsets, generate association rules with confidence above a _minimum confidence threshold_.

&nbsp;

The **Apriori algorithm** is a classic and fast mining algorithm belonging to the class of association rule mining algorithms.


`@script`
The main purpose of the association rule mining is to extract some induced rules from the itemsets over a transactional database. This association rule learning allows to discover interesting relationships between items.

To reach this goal, the mining process can be decomposed into two subtasks. First, the set of frequent items has to be generated from the database of transactions. An itemset is considered as frequent if its support satisfies a minimum threshold value. 
Second, using the set of frequent itemsets, rules with high confidence are extracted.

Note that the frequent itemset generation is much more computationally intensive than the generation of the associated rules.

The apriori algorithm is a classical algorithm in the family of association rule mining techniques. Other methods exists such as the ECLAT or the FP-growth algorithms.


---
## Back to the transactional data

```yaml
type: "TwoColumns"
key: "1813478938"
hide_title: false
```

`@part1`
| TID 	 | Transaction    |
|--------|----------------|
| 1      | {A, B, C, D}   |
| 2      | {A, B, D}      |
| 3      | {A, B}         |
| 4      | {B, C, D}      |
| 5      | {B, C}         |
| 6      | {C, D}         |
| 7      | {B, D}         |


`@part2`
![itemset_lattice](https://assets.datacamp.com/production/repositories/4926/datasets/d7997ea66cddc7c86cc7a84319b8cc640a6a4643/itemset_lattice.png =110){{2}}


`@script`
Recall from the previous chapters that we are working with a transactional dataset with 4 items, denoted as A, B, C and D to keep it easy. You can for instance imagine that these items represent items such as Bread, Butter, Cheese and Wine.

Remember that you are given a set of 7 transactions, all having a specific ID, the transaction ID. As you can see, the first transaction includes all 4 items, while the last one only includes items B and D.
Remember that our goal is to extract meaningful rules from this transactional dataset.

We are going to use the itemset lattice we have seen previously to illustrate how the apriori algorithm works. 
The itemset lattice displays all possible subsets of the original set of 4 items, organized as levels (starting from the empty set, 1-itemset, 2-itemsets, 3-itemsets and the 4-itemset).


---
## Idea behind the Apriori algorithm

```yaml
type: "FullSlide"
key: "0a6b566e2c"
```

`@part1`
**The apriori algorithm:**

- Level-wise, breath-first algorithm which counts transactions
- Generates candidate itemsets by exploiting the apriori principle


**Apriori principle**:

- If an itemset is frequent, then all of its subsets must also be frequent.
	- _e.g._ if {A,B} is frequent, then both {A} and {B} are frequent
- For an infrequent itemset, all its super-sets are infrequent.
	- _e.g._ if {A} is infrequent, then {A,B}, {A,C} and {A,B,C} are infrequent.


`@citations`
Agrawal and Srikant (1994)


`@script`
Level-wise approach as it traverses the itemset lattice one at a time, from frequent 1-itemset to the maximum size of frequent itemsets. 

Breadth-first algorithm searches the graph data structure. It starts at the tree root and explores all of the neighbor nodes at the present depth prior to moving on to the nodes at the next depth level.

The apriori algorithm generates candidate itemsets and measures support values in an efficient manner, via in particular the apriori principle.

The apriori principle states that:
- if an itemset is frequent, then all of its subsets must also be frequent. If the itemset {A,B} is frequent, then both {A} and {B} are frequent.
- Likewise, if an itemset is infrequent, then all its super-sets are infrequent. If itemset {A} is infrequent, then itemsets {A,B}, {A,C} and {A,B,C} are infrequent.


---
## Recap of the apriori algorithm

```yaml
type: "FullSlide"
key: "def53eb7c2"
```

`@part1`
Given a set of transactions, find all rules having at the same time

- support >= _minimum support threshold _
- confidence >= _minimum confidence threshold_


candidate generation
candidate pruning


`@script`
To avoid performing useless computations, it is useful to prune the rules without having to compute support and confidence values.


---
## Example: 1-itemset

```yaml
type: "TwoColumns"
key: "867fc1190a"
```

`@part1`
![item_set_lattice2](https://assets.datacamp.com/production/repositories/4926/datasets/2410e69c0cc858c2d970f83b1d36b98daf3f7b6e/itemset_lattice_1_r.png =80)


`@part2`
| TID 	 | Transaction                            |
|--------|----------------------------------------|
| 1      | {A, B, C, D}     	  |
| 2      | {A, B, D}              |
| 3      | {A, B}                |
| 4      | {B, C, D}         |
| 5      | {B, C}         |
| 6      | {C, D}         |
| 7     | {B, D}         |


`@citations`
Minimum support threshold = 3/7 = 0.42


`@script`
Let us reuse the lattice structure for that example. The search space of itemsets that need to be explored is exponentially large.

The apriori algorithm is a level-wise algorithm, meaning that we first start with the 1-itemset, the itemsets of size 1. 
We start by counting the number of occurences of A within the transactional dataset; in this case it appears 3 times. Similary, we scan the dataset to obtain the support of items B, C and D. In this case, all support values satisfy the minimum threshold which is set to 3. Therefore, all 4 itemsets are considered as frequent itemsets. This means that all 4 itemsets will be used to generate further candidate itemsets.


---
## Example: 2-itemsets

```yaml
type: "TwoColumns"
key: "0d9dabb41c"
```

`@part1`
![item_set_lattice3](https://assets.datacamp.com/production/repositories/4926/datasets/c47cab96e3fe4710525fcece8c9299b86feb52b7/itemset_lattice_2_r.png =80)


`@part2`
| TID 	 | Transaction                            |
|--------|----------------------------------------|
| 1      | {A, B, C, D}     	  |
| 2      | {A, B, D}              |
| 3      | {A, B}                |
| 4      | {B, C, D}         |
| 5      | {B, C}         |
| 6      | {C, D}         |
| 7      | {B, D}         |


`@citations`
Minimum support threshold = 3/7 = 0.42


`@script`
Once the first level (itemsets of length 1) has been analyzed, we move on to the second level (itemsets of length 2). All candidates are generated based on the frequent 1-itemsets. The support values of the 6 candidates are shown on the graph. Both itemsets (AC) and (AD) do not satisfy the minimum support threshold. From the apriori principle, this means that all supersets of (AC) and (AD) cannot be considered as frequent itemsets.


---
## Example: 3-itemsets

```yaml
type: "TwoColumns"
key: "5608bd3d6f"
```

`@part1`
![all_itemsets](https://assets.datacamp.com/production/repositories/4926/datasets/a95bdc27e475bdb72214862125361e91c975bb48/itemset_lattice_2b_r.png =75)


`@part2`
| TID 	 | Transaction                            |
|--------|----------------------------------------|
| 1      | {A, B, C, D}     	  |
| 2      | {A, B, D}              |
| 3      | {A, B}                |
| 4      | {B, C, D}         |
| 5      | {B, C}         |
| 6      | {C, D}         |
| 7      | {B, D}         |


`@citations`
Minimum support threshold = 3/7 = 0.42


`@script`
As both itemsets (AC) and (AD) are infrequent, they are colored in orange on the graph. Their respective supersets are as well colored in orange as these super-sets cannot be frequent itemsets (based on the apriori algorithm). It is therefore not necessary to scan the database for itemsets that are supersets of both infrequent itemsets, which substantially reduces the exploration space. 

The only 3-itemset candidate remaining is the itemset {BCD} which has a support of 2, meaning that it is an infrequent itemset.


---
## Example: frequent itemsets

```yaml
type: "TwoColumns"
key: "bc082b2107"
```

`@part1`
![item_set_lattice3](https://assets.datacamp.com/production/repositories/4926/datasets/dd85eef5519ce0bde80f9cf799c167d939b2a563/itemset_lattice_3_r.png =78)


`@part2`
| Itemset| Count  | Support |
|--------|--------|---------|
| {A}    | 3      | 0.42
| {B}    | 6      | 0.85
| {C}    | 4      | 0.57
| {D}    | 5      | 0.71
| {A,B}  | 3 	  | 0.42
| {B,C}  | 3      | 0.42
| {B,D}  | 4      | 0.57
| {C,D}  | 3      | 0.42


`@citations`
Minimum support threshold = 3/7 = 0.42


`@script`
The itemset {ABCD} at the bottom of the tree has been already discarded based on the apriori principle.
We finally obtain the setof frequent itemsets, shown in blue on the graph. This example has helped to understand how the apriori algorithm works to determine efficiently the frequent itemsets.


---
## Example: frequent itemsets with support 2

```yaml
type: "TwoColumns"
key: "27621b4294"
```

`@part1`
![itemset_supp_2](https://assets.datacamp.com/production/repositories/4926/datasets/004a4e14f2bb0d5a7fba96e9452d05d963b0c918/itemset_lattice_sup_2_r.png =75)


`@part2`
The lower the support, the longer the algorithm will take

**Advantages**

**Disadvantages**


`@citations`
Minimum support threshold = 2/7 = 0.28


`@script`



---
## A first try with the apriori

```yaml
type: "FullSlide"
key: "a6ae55fdc5"
```

`@part1`
**Transactional data**
```
inspect(head(trans,2))
```

```
    items     transactionID
[1] {A,B,C,D} 1            
[2] {A,B,D}   2            
```

**First call to the apriori function - frequent itemsets**
```r
support.all = apriori(trans, 
                      parameter = list(supp = 3/7, target="frequent itemsets"))
```


`@script`
Remember that the data we are working with is a transactional type of dataset, or at least one that can be coerced to such data type structure. The "transactional" object denoted "trans" contains the dataset shown in the previous slides. The first two transactions are displayed on the slide.

Let us start with a standard call of the apriori function to retrieve the list of frequent itemsets. We set as parameters of the function: 1. the minimum support threshold and 2. target as frequent itemset. We assign it to "support.all".


---
## Output of the apriori - frequent itemsets

```yaml
type: "TwoColumns"
key: "a9f5eecee4"
```

`@part1`
**Frequent itemsets**
```
> inspect(support.all)
    items support   count
[1] {A}   0.4285714 3    
[2] {C}   0.5714286 4    
[3] {D}   0.7142857 5    
[4] {B}   0.8571429 6    
[5] {A,B} 0.4285714 3    
[6] {C,D} 0.4285714 3    
[7] {B,C} 0.4285714 3    
[8] {B,D} 0.5714286 4
```


`@part2`
![item_set_lattice3](https://assets.datacamp.com/production/repositories/4926/datasets/dd85eef5519ce0bde80f9cf799c167d939b2a563/itemset_lattice_3_r.png =78)


`@script`
By inspecting the object just created, we obtain a dataframe containing the frequent itemsets with their respective support and count (all counts are indeed greater or equal than 3). The R output corresponds exactly to the itemset lattice displayed on the right hand side of the slide.


---
## Extracting rules with the apriori function

```yaml
type: "FullSlide"
key: "300fe878ef"
```

`@part1`
**Parameter**: the mining parameters change the characteristics of the mined itemsets or rules.
- Support = 3/7
- Confidence = 60%
- Minimum length of rule = 2

```
?APparameter
```
**Standard call to the apriori function for rule generation**
```
rules.all = apriori(trans,parameter = list(supp = 3/7, conf = 0.6, minlen = 2))
inspect(rules.all)
```


`@script`
After obtaining the set of frequent itemsets, we use the apriori function to generate association rules from the transactional data.
The main parameters of the apriori function are the support, confidence and minimum length (of the rule). More information about available parameters can be found by typing ?APparameter.
Let us try a first call of the apriori where the inputs of the function are 1. the transactional dataset 2. specified parameters, passed as a list. Finally, we use the inspect function to examine the set of generated rules.


---
## Extracting rules: output

```yaml
type: "FullSlide"
key: "7acad20eeb"
```

`@part1`
**Summary of rules** 
```
summary(rules.all)
```


`@script`



---
## Playing with association rules: Sorting

```yaml
type: "FullSlide"
key: "b5334a1e0c"
```

`@part1`
Sorting rules by different measures (support, confidence, lift,...)

```
rules.sorted = sort(rules, by = "support")
inspect(rules.sorted)
```

```
    lhs    rhs support   confidence lift      count
[1] {D} => {B} 0.5714286 0.8000000  0.9333333 4    
[2] {B} => {D} 0.5714286 0.6666667  0.9333333 4    
[3] {A} => {B} 0.4285714 1.0000000  1.1666667 3    
[4] {C} => {D} 0.4285714 0.7500000  1.0500000 3    
[5] {D} => {C} 0.4285714 0.6000000  1.0500000 3    
[6] {C} => {B} 0.4285714 0.7500000  0.8750000 3
```


`@script`
It is also possible to sort rules according to a specific measure, such as support, confidence, lift,...
Use the sort function, passing as argument the apriori object created previously as well as the measure used for sorting.


---
## Playing with association rules: Subsetting

```yaml
type: "FullSlide"
key: "8a43969714"
```

`@part1`
Subsetting rules based on: 

- specific itemsets (as antecedent or precedent)
- measure (support, confidence, lift, ...)

```
rules.sub <- subset(rules.all, subset = rhs %in% "B" & lift > 1)
inspect(rules.sub)
```

```
    lhs    rhs support   confidence lift     count
[1] {A} => {B} 0.4285714 1          1.166667 3  
```


`@script`
With the function subset, it is possible to subset from the set of original rules. Filters used for subsetting can be specific itemset (rhs or lhs) and/or measures such as support, confidence, lift.

For instance, in the example shown on the slide, we are interested in all rules where the right-hand side (the consequent of the rule) is B and for which the lift is greater than 1.
The only rule satisfying these constraints is A implies B.


---
## Arguments of the apriori function

```yaml
type: "FullSlide"
key: "0472819503"
```

`@part1`
**Arguments to the function**

- **parameter**
- **control**: influence the performance of the algorithm (e.g. ???)
- **appearance**


```
rules = apriori(trans,
				parameter = list(supp=0.05, conf=0.5, minlen=2),
                control = list(verbose=F),
                appearance = list(rhs="A", default = "lhs"))
```


`@script`
Now let us go through the main arguments of the apriori function which allow to steer the output and the set of extracted rules.

Sometimes, you may be interested in a specific rule, such as imposing that the consequent of the rule is A.


---
## Rule redundancy

```yaml
type: "FullSlide"
key: "d9c318cd97"
```

`@part1`
**Set of generated rules**

```
rules = apriori(trans,control = list(verbose=F),
                parameter = list(supp=0.05, conf=0.5, minlen=2),
                appearance = list(rhs="A", default = "lhs"))
 
```

**Set of pruned rules (non-redundant)**

```
subset.mat = is.subset(rules, rules)
redundant = colSums(subset.mat)>1
rules.pruned = rules[!redundant]
```


`@script`
There are often too many association rules discovered from a transactional dataset. To illustrate this, let us call again the apriori function with different parameters. 

Given the large number of rules extracted from the apriori function, it is necessary to get an overview of redundant rules and identify the interesting rules.


---
## Rule redundancy

```yaml
type: "FullSlide"
key: "3a81639fe1"
```

`@part1`
**Comparing rules and non-redundant rules**

```
> inspect(rules) 
    lhs        rhs support   confidence lift     count
[1] {B}     => {A} 0.4285714 0.5        1.166667 3    
[2] {B,D}   => {A} 0.2857143 0.5        1.166667 2    
[3] {B,C,D} => {A} 0.1428571 0.5        1.166667 1 
```

```
> inspect(rules.pruned)
    lhs    rhs support   confidence lift     count
[1] {B} => {A} 0.4285714 0.5        1.166667 3 
```


`@script`
How redundant is the set of extracted rules?

In this example, rule #2 and rule #3 provide no extra knowledge in addition to rule #1.
When a rule (such as #2 and #3) is a super rule of another rule (#1) and the former has the same or a lower lift, the former rule is considered to be redundant.
In this case, if "B implies A", it is natural that both "BD" and "BCD" also imply A (given that the lift is the same). Therefore the rule "B implies A" is a non-redundant rule, the one considered as the interesting one.


---
## Let's practice!

```yaml
type: "FinalSlide"
key: "cbc4d2b5e4"
```

`@script`
Now it is your turn to practice the skills you have learned on the Online Retail dataset. You will use the transactional dataset to apply the apriori algorithm. Happy shopping!

