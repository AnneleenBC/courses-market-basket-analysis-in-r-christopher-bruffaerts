---
title: Insert title here
key: 7774a6cccae2fd3fe8d17311854e7472

---
## Screencast

```yaml
type: "TitleSlide"
key: "be5c7a4595"
```

`@lower_third`

name: Christopher Bruffaerts
title: Lecturer in Data science, EPFL


`@script`
In this lesson, we cover the most popular algorithm of the class of association rule mining methods, the apriori algorithm.


---
## Association rule mining

```yaml
type: "FullSlide"
key: "c825ec5187"
```

`@part1`
**Association rule mining** allows to discover interesting relationships between items in a large transactional database.

This mining task can be divided into two subtasks:

- **Frequent itemset generation**: determine all frequent itemsets of a potentially large database of transactions. An itemset is said to be frequent if it satisfies a _minimum support threshold_.

- **Rule generation**: from the above frequent itemsets, generate association rules with confidence above a _minimum confidence threshold_.

&nbsp;

The **Apriori algorithm** is a classic and fast mining algorithm belonging to the class of association rule mining algorithms.


`@script`
To extract interesting rules from the transactional dataset, the association rule mining can be decomposed into two subtasks. First, the set of frequent items is generated and second, rules with high confidence are extracted from this set of frequent itemsets.

An itemset is considered as frequent if its support satisfies a minimum threshold value. 

The apriori algorithm is an efficient association rule mining technique.


---
## Idea behind the Apriori algorithm

```yaml
type: "FullSlide"
key: "0a6b566e2c"
```

`@part1`
**The apriori algorithm:**

- Level-wise, breath-first algorithm which counts transactions
- Generates candidate itemsets by exploiting the apriori principle

**Apriori principle**:

- If an itemset is frequent, then all of its subsets must also be frequent.
	- _e.g._ if {A,B} is frequent, then both {A} and {B} are frequent
- For an infrequent itemset, all its super-sets are infrequent.
	- _e.g._ if {A} is infrequent, then {A,B}, {A,C} and {A,B,C} are infrequent.


`@citations`
Agrawal and Srikant (1994)


`@script`
The apriori algorithm is a level-wise, breadth-first algorithm, This means that it will go through the entire itemset lattice (starting from the 1-itemset), explores and counts all of the neighbour nodes and then moves to the 2-itemsets.

The algorithm makes use of the apriori principle to generate candidate itemsets in an efficient manner: 

- If an itemset such as {A,B} is frequent, then both subsets {A} and {B} are frequent.
- Likewise, if an itemset such as {A} is infrequent, then all its super-sets such as {A,B}, {A,C} and {A,B,C} are infrequent.


---
## Recap of the apriori algorithm

```yaml
type: "FullSlide"
key: "def53eb7c2"
```

`@part1`
Given a set of transactions, find all rules having at the same time

- support >= _minimum support threshold _
- confidence >= _minimum confidence threshold_


candidate generation
candidate pruning

It has got this odd name because it uses ‘prior’ knowledge of frequent itemset properties.


`@script`
To avoid performing useless computations, it is useful to prune the rules without having to compute support and confidence values.


---
## Example: 1-itemset

```yaml
type: "TwoColumns"
key: "867fc1190a"
```

`@part1`
![item_set_lattice2](https://assets.datacamp.com/production/repositories/4926/datasets/2410e69c0cc858c2d970f83b1d36b98daf3f7b6e/itemset_lattice_1_r.png =80)


`@part2`
| TID 	 | Transaction                            |
|--------|----------------------------------------|
| 1      | {A, B, C, D}     	  |
| 2      | {A, B, D}              |
| 3      | {A, B}                |
| 4      | {B, C, D}         |
| 5      | {B, C}         |
| 6      | {C, D}         |
| 7     | {B, D}         |


`@citations`
Minimum support threshold = 3/7 = 0.42


`@script`
To illustrate how the apriori algorithm works, we use the previous transactional dataset and the corresponding itemset lattice. 
 
The algorithm starts by counting the number of occurences of A; in this case it appears 3 times. Similarly, we scan the dataset to obtain the support of items B, C and D. All itemsets are frequent as their support values satisfy the minimum support threshold set to 3. These 4 itemsets are used to generate further candidate itemsets.


---
## Example: 2-itemsets

```yaml
type: "TwoColumns"
key: "0d9dabb41c"
```

`@part1`
![item_set_lattice3](https://assets.datacamp.com/production/repositories/4926/datasets/c47cab96e3fe4710525fcece8c9299b86feb52b7/itemset_lattice_2_r.png =80)


`@part2`
| TID 	 | Transaction                            |
|--------|----------------------------------------|
| 1      | {A, B, C, D}     	  |
| 2      | {A, B, D}              |
| 3      | {A, B}                |
| 4      | {B, C, D}         |
| 5      | {B, C}         |
| 6      | {C, D}         |
| 7      | {B, D}         |


`@citations`
Minimum support threshold = 3/7 = 0.42


`@script`
We move on to itemsets of length 2. All candidates are generated based on the frequent 1-itemsets. Both itemsets (AC) and (AD) do not satisfy the minimum support threshold meaning from the apriori principle that all their supersets are as well infrequent itemsets.


---
## Example: 3-itemsets

```yaml
type: "TwoColumns"
key: "5608bd3d6f"
```

`@part1`
![all_itemsets](https://assets.datacamp.com/production/repositories/4926/datasets/a95bdc27e475bdb72214862125361e91c975bb48/itemset_lattice_2b_r.png =75)


`@part2`
| TID 	 | Transaction                            |
|--------|----------------------------------------|
| 1      | {A, B, C, D}     	  |
| 2      | {A, B, D}              |
| 3      | {A, B}                |
| 4      | {B, C, D}         |
| 5      | {B, C}         |
| 6      | {C, D}         |
| 7      | {B, D}         |


`@citations`
Minimum support threshold = 3/7 = 0.42


`@script`
Both infrequent itemsets (AC) and (AD) and their respective super-sets are colored in orange on the graph. The exploration space has therefore been substantially reduced.

The remaining 3-itemset candidate is the itemset {BCD} with support of 2, meaning that it is an infrequent itemset.


---
## Example: frequent itemsets

```yaml
type: "TwoColumns"
key: "bc082b2107"
```

`@part1`
![item_set_lattice3](https://assets.datacamp.com/production/repositories/4926/datasets/dd85eef5519ce0bde80f9cf799c167d939b2a563/itemset_lattice_3_r.png =78)


`@part2`
| Itemset| Count  | Support |
|--------|--------|---------|
| {A}    | 3      | 0.42
| {B}    | 6      | 0.85
| {C}    | 4      | 0.57
| {D}    | 5      | 0.71
| {A,B}  | 3 	  | 0.42
| {B,C}  | 3      | 0.42
| {B,D}  | 4      | 0.57
| {C,D}  | 3      | 0.42


`@citations`
Minimum support threshold = 3/7 = 0.42


`@script`
We finally obtain the set of frequent itemsets, shown in blue on the graph. This example helps to understand how the apriori algorithm generates efficiently the frequent itemsets.


---
## A first try with the apriori

```yaml
type: "FullSlide"
key: "a6ae55fdc5"
```

`@part1`
**Transactional data**
```
inspect(head(trans,2))
```

```
    items     transactionID
[1] {A,B,C,D} 1            
[2] {A,B,D}   2            
```

**First call to the apriori function - frequent itemsets**
```r
support.all = apriori(trans, 
                      parameter = list(supp = 3/7, target="frequent itemsets"))
```


`@script`
To retrieve the list of frequent itemsets in R, we start with a standard call of the apriori function. The arguments are the transactional dataset as well as the following parameters: 1. the minimum support threshold denoted as "supp" and 2. the "target" as frequent itemset.


---
## Output of the apriori - frequent itemsets

```yaml
type: "TwoColumns"
key: "a9f5eecee4"
```

`@part1`
**Frequent itemsets**
```
> inspect(support.all)
    items support   count
[1] {A}   0.4285714 3    
[2] {C}   0.5714286 4    
[3] {D}   0.7142857 5    
[4] {B}   0.8571429 6    
[5] {A,B} 0.4285714 3    
[6] {C,D} 0.4285714 3    
[7] {B,C} 0.4285714 3    
[8] {B,D} 0.5714286 4
```


`@part2`
![item_set_lattice3](https://assets.datacamp.com/production/repositories/4926/datasets/dd85eef5519ce0bde80f9cf799c167d939b2a563/itemset_lattice_3_r.png =78)


`@script`
By inspecting the object, we obtain a dataframe containing the set of frequent itemsets which corresponds to the itemset lattice we built earlier.


---
## Extracting rules with the apriori function

```yaml
type: "FullSlide"
key: "300fe878ef"
```

`@part1`
**Parameter**: the mining parameters change the characteristics of the mined itemsets or rules.
- Support = 3/7
- Confidence = 60%
- Minimum length of rule = 2

**Call to the apriori function for rule generation with specific arguments**
```
rules.all = apriori(trans,
				parameter = list(supp=3/7, conf=0.6, minlen=2),
                control = list(verbose=F),
                appearance = list(rhs="A", default = "lhs"))
inspect(rules.all)
```


`@script`
To generate association rules, we call the apriori function with as argument the transactional datasets as well as the following parameters passed in a list:
- minimum support threshold
- confidence threshold
- minimum length of rules 

to steer the output and the set of extracted rules.

Sometimes, you may be interested in a specific rule, such as imposing that the consequent of the rule is A.


---
## Extracting rules: output

```yaml
type: "FullSlide"
key: "7acad20eeb"
```

`@part1`
**Inspecting the rules** 

```
> inspect(rules.all)
    lhs    rhs support   confidence lift      count
[1] {A} => {B} 0.4285714 1.0000000  1.1666667 3    
[2] {C} => {D} 0.4285714 0.7500000  1.0500000 3    
[3] {D} => {C} 0.4285714 0.6000000  1.0500000 3    
[4] {C} => {B} 0.4285714 0.7500000  0.8750000 3    
[5] {D} => {B} 0.5714286 0.8000000  0.9333333 4    
[6] {B} => {D} 0.5714286 0.6666667  0.9333333 4 
```


`@script`
The "inspect" function allows to extract the generate rules with their corresponding measures.
For instance, the first rule states that "A implies B" for a support of 42% and a confidence of 100%.


---
## Rule redundancy

```yaml
type: "FullSlide"
key: "d9c318cd97"
```

`@part1`
**Set of generated rules**

```
rules = apriori(trans,control = list(verbose=F),
                parameter = list(supp=0.05, conf=0.5, minlen=2),
                appearance = list(rhs="A", default = "lhs"))
 
```

**Set of pruned rules (non-redundant)**

```
subset.mat = is.subset(rules, rules)
redundant = colSums(subset.mat)>1
rules.pruned = rules[!redundant]
```


`@script`
There are often too many association rules discovered from a transactional dataset. 
We use the "is.subset" function to figure out which items are subsets of which items among all possible itemsets. This allows to obtain the list of redundant rules, and exclude the redundant rules from the set or original extracted rules.


---
## Rule redundancy

```yaml
type: "FullSlide"
key: "3a81639fe1"
```

`@part1`
**Comparing extracted rules and non-redundant rules**

```
> inspect(rules) 
    lhs        rhs support   confidence lift     count
[1] {B}     => {A} 0.4285714 0.5        1.166667 3    
[2] {B,D}   => {A} 0.2857143 0.5        1.166667 2    
[3] {B,C,D} => {A} 0.1428571 0.5        1.166667 1 
```

```
> inspect(rules.pruned)
    lhs    rhs support   confidence lift     count
[1] {B} => {A} 0.4285714 0.5        1.166667 3 
```


`@script`
In this example, rule #2 and rule #3 are considered redundant rules as they provide no extra knowledge in addition to rule #1.

When a rule (such as #2 and #3) is a super rule of another rule (#1) and the former has the same or a lower lift, the former rule is considered to be redundant. "B implies A" is a non-redundant and informative rule.


---
## Let's practice!

```yaml
type: "FinalSlide"
key: "cbc4d2b5e4"
```

`@script`
Now it is your turn to practice the skills you have learned on the Online Retail dataset. You will apply the apriori algorithm on this dataset. Happy shopping!

