---
title: Insert title here
key: 7774a6cccae2fd3fe8d17311854e7472

---
## Screencast

```yaml
type: "TitleSlide"
key: "be5c7a4595"
```

`@lower_third`

name: Christopher Bruffaerts
title: Lecturer in Data science, EPFL


`@script`
In this lesson, we are going to cover one of the algorithm behind the association rule mining, the so-called apriori algorithm. We are first going to understand the rationale behind this algorithm and afterwards see how to apply it with the arules package.


---
## Association rule mining

```yaml
type: "FullSlide"
key: "c825ec5187"
```

`@part1`
**Association rule mining** allows to discover interesting relationships between items in a large transactional database.

This mining task can be divided into two subtasks:

- **Frequent itemset generation**: determine all frequent itemsets of a potentially large database of transactions. An itemset is said to be frequent if it satisfies a _minimum support threshold_.

- **Rule generation**: extract all the high-confidence rules from the frequent itemsets found in the previous steps.

&nbsp;

The **Apriori algorithm** is one of the algorithm belonging to the category of association rule mining.


`@script`
The main purpose of the association rule mining is to extract some induced rules from the itemsets over a transactional database. This _association rule learning_ allows to discover ....

To reach this goal, the mining process can be decomposed into two subtasks. First, the set of frequent items has to be generated from the database of transactions. An itemset is considered as frequent if its support satisfies a minimum threshold value. 
Second, using the frequent itemsets, the set of high confidence rules is extracted.

The computational requirements for frequent itemset generation are more expensive than those of rule generation.


---
## Back to the transactional data

```yaml
type: "TwoColumns"
key: "1813478938"
hide_title: false
```

`@part1`
| TID 	 | Transaction    |
|--------|----------------|
| 1      | {A, B, C, D}   |
| 2      | {A, B, D}      |
| 3      | {A, B}         |
| 4      | {B, C, D}      |
| 5      | {B, C}         |
| 6      | {C, D}         |
| 7      | {B, D}         |


`@part2`
![itemset_lattice](https://assets.datacamp.com/production/repositories/4926/datasets/d7997ea66cddc7c86cc7a84319b8cc640a6a4643/itemset_lattice.png =110){{2}}


`@script`
Recall from the previous chapters that we are working with a transactional dataset with 4 items, denoted as A, B, C and D to keep it easy. You can for instance imagine that these items represent items such as Bread, Butter, Cheese and Wine.

Remember that you are given a set of 7 transactions, all having a specific ID, the transaction ID. As you can see, the first transaction/basket includes all 4 items, while the last one only includes items B and D.


We are going to use the itemset lattice we have seen previously to illustrate how the association rule mining works, and in particular how the apriori algorithm functions. The itemset lattice displays all possible subsets of the original set of 4 items, organized as levels (starting from the empty set, 1-itemset, 2-itemsets, 3-itemsets and the 4-itemset).


---
## Idea behind the Apriori algorithm

```yaml
type: "FullSlide"
key: "0a6b566e2c"
```

`@part1`
Add the different steps


`@script`



---
## Apriori principle

```yaml
type: "FullSlide"
key: "a9578f79ad"
```

`@part1`
Idea/rationale

- Bottom-up approach
- Level-wise, breath-first algorithm which counts transactions


**Apriori principle**:

If an itemset is frequent, then all of its subsets must also be frequent.

Anti-monotonicity


`@script`



---
## Example: 1-itemset

```yaml
type: "TwoColumns"
key: "867fc1190a"
```

`@part1`
![item_set_lattice2](https://assets.datacamp.com/production/repositories/4926/datasets/2410e69c0cc858c2d970f83b1d36b98daf3f7b6e/itemset_lattice_1_r.png =80)


`@part2`
| TID 	 | Transaction                            |
|--------|----------------------------------------|
| 1      | {A, B, C, D}     	  |
| 2      | {A, B, D}              |
| 3      | {A, B}                |
| 4      | {B, C, D}         |
| 5      | {B, C}         |
| 6      | {C, D}         |
| 7     | {B, D}         |


`@citations`
Minimum support threshold = 3/7 = 0.42


`@script`
The apriori algorithm is a level-wise algorithm, meaning that we first start with the 1-itemset, the itemsets of size 1. 
We start by counting the number of occurences of A within the transactional dataset; in this case it appears 3 times. Similary, we scan the databset to obtain the support of items B, C and D. In this case, all support values satisfy the minimum threshold which is set to 3. Therefore, all 4 itemsets are considered as frequent itemsets. This means that all 4 itemsets will be used to generate further candidate itemsets.


---
## Example: 2-itemsets

```yaml
type: "TwoColumns"
key: "0d9dabb41c"
```

`@part1`
![item_set_lattice3](https://assets.datacamp.com/production/repositories/4926/datasets/c47cab96e3fe4710525fcece8c9299b86feb52b7/itemset_lattice_2_r.png =80)


`@part2`
| TID 	 | Transaction                            |
|--------|----------------------------------------|
| 1      | {A, B, C, D}     	  |
| 2      | {A, B, D}              |
| 3      | {A, B}                |
| 4      | {B, C, D}         |
| 5      | {B, C}         |
| 6      | {C, D}         |
| 7      | {B, D}         |


`@citations`
Minimum support threshold = 3/7 = 0.42


`@script`
Once the first level (itemsets of length 1) has been analyzed, we move on to the second level (itemsets of length 2). All candidates are generated from the itemsets of the first level. The support values of the 6 candidates are shown on the graph. Both itemsets (AC) and (AD) do not satisfy the minimum support threshold. From the apriori principle, this means that all supersets of (AC) and (AD) cannot be considered as frequent itemsets.


---
## Example: 3-itemsets

```yaml
type: "TwoColumns"
key: "5608bd3d6f"
```

`@part1`
![all_itemsets](https://assets.datacamp.com/production/repositories/4926/datasets/a95bdc27e475bdb72214862125361e91c975bb48/itemset_lattice_2b_r.png =75)


`@part2`
| TID 	 | Transaction                            |
|--------|----------------------------------------|
| 1      | {A, B, C, D}     	  |
| 2      | {A, B, D}              |
| 3      | {A, B}                |
| 4      | {B, C, D}         |
| 5      | {B, C}         |
| 6      | {C, D}         |
| 7      | {B, D}         |


`@citations`
Minimum support threshold = 3/7 = 0.42


`@script`
Both itemsets (AC) and (AD) are colored in orange, as well as their supersets. It is not necessary for the algorithm to scan the database for itemsets that are supersets of both infrequent itemsets.


---
## Example: frequent itemsets

```yaml
type: "TwoColumns"
key: "bc082b2107"
```

`@part1`
![item_set_lattice3](https://assets.datacamp.com/production/repositories/4926/datasets/dd85eef5519ce0bde80f9cf799c167d939b2a563/itemset_lattice_3_r.png =78)


`@part2`
| Itemset| Count  | Support |
|--------|--------|---------|
| {A}    | 3      | 0.42
| {B}    | 6      | 0.85
| {C}    | 4      | 0.57
| {D}    | 5      | 0.71
| {A,B}  | 3 	  | 0.42
| {B,C}  | 3      | 0.42
| {B,D}  | 4      | 0.57
| {C,D}  | 3      | 0.42


`@citations`
Minimum support threshold = 3/7 = 0.42


`@script`
Both itemsets (AC) and (AD) are colored in orange, as well as their supersets. It is not necessary for the algorithm to scan the database for itemsets that are supersets of both infrequent itemsets.


---
## Example: frequent itemsets with support 2

```yaml
type: "TwoColumns"
key: "27621b4294"
```

`@part1`
![itemset_supp_2](https://assets.datacamp.com/production/repositories/4926/datasets/004a4e14f2bb0d5a7fba96e9452d05d963b0c918/itemset_lattice_sup_2_r.png =75)


`@part2`
The lower the support, the longer the algorithm will take

**Advantages**

**Disadvantages**


`@citations`
Minimum support threshold = 2/7 = 0.28


`@script`



---
## A first try with the apriori

```yaml
type: "FullSlide"
key: "a6ae55fdc5"
```

`@part1`
**Transactional data**
```
inspect(head(trans,2))
```

```
    items     transactionID
[1] {A,B,C,D} 1            
[2] {A,B,D}   2            
```

**First call to the apriori function - frequent itemsets**
```r
support.all = apriori(trans, 
                      parameter = list(supp = 3/7, target="frequent itemsets"))
```


`@script`
Remember that the data we are working with is a transactional type of dataset, or one that can be coerced to such data type structure. The "trans" object contains the dataset shown in the previous slides.

It is always a good practice to check the documentation of a R function before working with it. 

Let us start with a standard call of the apriori function with default parameters. Afterwards, we will play with the various parameters of the function. 
In this first call of the function, we retrieve the list of frequent itemsets.


---
## Output of the apriori - frequent itemsets

```yaml
type: "TwoColumns"
key: "a9f5eecee4"
```

`@part1`
**Frequent itemsets**
```
> inspect(support.all)
    items support   count
[1] {A}   0.4285714 3    
[2] {C}   0.5714286 4    
[3] {D}   0.7142857 5    
[4] {B}   0.8571429 6    
[5] {A,B} 0.4285714 3    
[6] {C,D} 0.4285714 3    
[7] {B,C} 0.4285714 3    
[8] {B,D} 0.5714286 4
```


`@part2`
![item_set_lattice3](https://assets.datacamp.com/production/repositories/4926/datasets/dd85eef5519ce0bde80f9cf799c167d939b2a563/itemset_lattice_3_r.png =78)


`@script`



---
## Extracting rules with the apriori function

```yaml
type: "FullSlide"
key: "300fe878ef"
```

`@part1`
**Parameter**: the mining parameters change the characteristics of the mined itemsets or rules.
- Support = 3/7
- Confidence = 60%
- Minimum length of rule = 2

```
?APparameter
```
**Standard call to the apriori function for rule generation**
```
rules.all = apriori(trans,parameter = list(supp = 3/7, conf = 0.6, minlen = 2))
inspect(rules.all)
```


`@script`



---
## Apriori output

```yaml
type: "FullSlide"
key: "7acad20eeb"
```

`@part1`
**Summary of rules** 
```
summary(rules.all)
```


`@script`



---
## Playing with rules: Sorting

```yaml
type: "FullSlide"
key: "b5334a1e0c"
```

`@part1`
Sorting rules by different measures (support, confidence, lift,...)

```
rules.sorted = sort(rules, by = "support")
inspect(rules.sorted)
```

```
    lhs    rhs support   confidence lift      count
[1] {D} => {B} 0.5714286 0.8000000  0.9333333 4    
[2] {B} => {D} 0.5714286 0.6666667  0.9333333 4    
[3] {A} => {B} 0.4285714 1.0000000  1.1666667 3    
[4] {C} => {D} 0.4285714 0.7500000  1.0500000 3    
[5] {D} => {C} 0.4285714 0.6000000  1.0500000 3    
[6] {C} => {B} 0.4285714 0.7500000  0.8750000 3
```


`@script`



---
## Playing with rules: Subsetting

```yaml
type: "FullSlide"
key: "8a43969714"
```

`@part1`
Subsetting rules based on: 

- itemsets (as antecedent or precedent)
- measure (support, confidence, lift, ...)


```
rules.sub <- subset(rules.all, subset = rhs %in% "B" & lift > 1)
inspect(rules.sub)
```

```
    lhs    rhs support   confidence lift     count
[1] {A} => {B} 0.4285714 1          1.166667 3  
```


`@script`



---
## Parameters and controls of the apriori

```yaml
type: "FullSlide"
key: "0472819503"
```

`@part1`
- **Control**: influence the performance of the algorithm (e.g. ???)
- **Appearance**


```
?apriori
?APparameter
?APcontrol
?APappearance
```


`@script`



---
## Specific rules

```yaml
type: "FullSlide"
key: "753b30f2db"
```

`@part1`
Rules for specific items

```
C_rules <- apriori(data=trans, 
                   parameter=list(supp=3/7,conf=0.2, minlen=2),
                   appearance = list(rhs="C"),
                   control = list (verbose=F))
inspect(C_rules)
```

```
    lhs    rhs support   confidence lift  count
[1] {D} => {C} 0.4285714 0.6        1.050 3    
[2] {B} => {C} 0.4285714 0.5        0.875 3 
```


`@script`
Sometimes, you may be interested in a specific rule, such as..


---
## Rule redundancy

```yaml
type: "FullSlide"
key: "2686cd1e37"
```

`@part1`
**What is a redundant rule?**


**How redundant is the set of extracted rules?**

```
subset.mat = is.subset(rules.all, rules.all)
redundant = colSums(subset.mat)>1
which(redundant)
```

```
rules.pruned = rules.all[!redundant]
inspect(rules.pruned)
```


`@script`
Given the large number of rules extracted from the apriori function, it is necessary to get an overview of redundant rules. What do we mean by "redundant" rule?


---
## Rule redundancy

```yaml
type: "TwoColumns"
key: "ae84245db6"
```

`@part1`
set of all rules


`@part2`
set of rules minus the redundant ones


`@script`



---
## Apriori options

```yaml
type: "FullSlide"
key: "c76f2b1c05"
```

`@part1`
List all possible options available in arules with apriori

```
summary()
length()
items()
write()
write.pmml()
```


`@script`



---
## Let's practice!

```yaml
type: "FinalSlide"
key: "cbc4d2b5e4"
```

`@script`
Now it is your turn to practice the skills you have learned on the Online Retail dataset. You will use the transactional dataset to apply the apriori algorithm. Happy shopping!

